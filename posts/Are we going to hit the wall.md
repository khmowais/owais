---
title: "Are we going to hit the wall?"
date: "2025-11-25"
tags: ["AI", "philosophy", "internet", "technology", "LLMs", "CS", "computers", "culture"]
---
### **Machines started talking to themselves**
In the early days of web, web was raw and alive. If you talked to someone online, you were certainly talking to a person, an opinionated, typo prone and unpredictable human. Internet back then was a collective diary of human consciousness. Every post, argument and badly formatted web page showed the work of natural human minds.

Then came the first chatbots. Business websites began experimenting with tiny digital receptionists who were stiff, rule based things that gave you menu options instead of sentences. Nobody was fooled. You knew you were talking to a bot because it felt like talking to an ATM machine. Press 1 for service x and 2 for service y etc. 

But now, something far stranger is happening. We are moving in an era where much of what we read online isn’t written by real people anymore.  A growing share of blogs, marketing posts, reviews, product descriptions, social media captions etc are written by LLMs. These models are trained on the entire web… which increasingly includes their own outputs.

This creates a feedback loop that’s frightening.  
Humans wrote on internet.
LLMs learnt from internet.  
LLMs start predicting next words/pixels.
Content on internet starts being generated by LLMs.
LLMs now try to learn new things on internet.  
LLMs now learn again from that same internet.
LLMs are learning from themselves.

### **The Copy of a Copy Problem**

Think of it like photocopying a photocopy. The first copy looks fine, but each generation gets blurrier. The edges soften. Details disappear. The noise becomes the signal. Humans and LLMs think differently. When i read a text written by an LLM,i can instantly feel it and this is with the training data of real human work. Also it’s not that the machine is wrong; it’s that it’s too right. Too tidy. Too well behaved. Now imagine training future models entirely on this kind of text. If the results are so bad with rich training dataset, what will happen if the training data is itself very bad. The model will start to believe this politeness is what human thought looks like and then add its badness to it. It will forget our realities which are chaos, humor, sudden irrational leaps that lead to creativity etc. This graphic below i made quickly and poorly, red link is the dangerous link and thickness of arrow shows contribution. Everyone consumes web but a few contribute to it. etc
[graphic](https://github.com/khmowais/owais/blob/main/posts/LLM_internet.png)
The more the machine talks to itself, the less human it becomes and it will believe it is becoming more human, as its training data is filled with its own words.

### **Understanding to simulations**

Early AI research was about understanding the human mind. Modern LLMs don’t understand instead they simulate. They create plausible shadows of human thinking without knowing the light source. When the data they’re fed comes from human experience, those shadows can often align fairly well with reality but when they’re trained on their own simulations, they drift. The map no longer fits the reality. At scale, this means our digital world begins to lose contact with the physical one. The internet will become a simulation of a simulation.

### **Why This Matters**

When every word online could be generated by a model, we lose our direction. We stop asking “Who said this?” because there is no who. The danger isn’t just bad information. It’s meaning decay, the slow death of genuine human expression replaced by algorithmically averaged sentences that have no insight. The endgame is will be a hollow web, a language store where everything sounds right but nothing means anything.

### **A Path Forward**

Avoiding this collapse doesn’t mean abandoning AI. It means grounding it again in the real.  Feed models with verified, sensory rich, human originated data. Keep humans in the loop. Teach AIs to learn from the world, not just from their own reflections. The future of intelligence(artificial or otherwise) depends on remembering that thought isn’t a pattern of words. It’s an act of seeing, feeling, and choosing.

#### A similar Joke i found on internet:

The Indians on a remote reservation in Oklahoma asked their new chief if the coming winter was going to be mild or cold. The chief looked to the sky, but he couldn't tell what the winter was going to be like. To be on the safe side, he told the tribe, The winter is indeed going to be cold and all members of the village shall collect firewood to be prepared. But being a practical leader, after several days he got an idea. He went to their phone booth and called the National Weather Service and asked,"Is the coming winter going to be cold?" The meteorologist at the Weather Service responded, It looks like this winter is going to be quite cold. So the chief went back to his people and told them to collect even more firewood to be prepared. A week later, he called the National Weather Service again. Does it still look like it is going to be a very cold winter? Yes, it is going to be a very cold winter. The chief again went back to his people and ordered them to collect every piece of firewood they could find. Two weeks later, the chief called the National Weather Service again. Are you absolutely sure that this winter is going to be very cold? It is looking more and more like it is going to be one of the coldest winters we've ever seen. How can you be so sure? The Indians are collecting a shit load of firewood.


I had this md file wriiten but not published and computerphile just did a video on same topic further proving my point. Now i am gonna publish it.
![[file_creation_date.png]]
![[computerphile.png]]
	
